# ğŸ•¸ï¸ Upwork Job Scraper âœ AirtableÂ VisualÂ Dashboard

> **Oneâ€‘click insight into the freelance projects that matter to me.**

---

## ğŸš€ Motivation & Objectives

* **Cut through noiseÂ & decision fatigue**: Upworkâ€™s interface buries the gigs I actually care about; this project surfaces them instantly.
* **Respect UpworkÂ ToS**: All HTML pages are **manually** downloadedâ€”no automated scraping of the live site.
* **Centralised review**: Push cleaned job data to Airtable so I can tag, score, and track leads in one place.
* **Daily habit loop**: A lightweight workflow I can run every morning in under 5Â minutes.

---

## ğŸ”„ Proposed Workflow (Visual)

```mermaid
flowchart TD
    Step1[Create saved searches list] --> Step2[Manual HTML download daily]
    Step2 --> Step3[Render pages with Playwright and extract Nuxt JSON]
    Step3 --> Step4[Combine JSON into dataframe]
    Step4 --> Step5[Push postings to Airtable]
    Step5 --> Step6[Delete HTML files]
```

---

## ğŸ—‚ï¸ Project & File Structure

```text
upwork_scraper_airtable/
â”œâ”€â”€ environment.yml            # Conda environment spec
â”œâ”€â”€ README.md                  # <- you are here
â”œâ”€â”€ .gitignore                 # Exclude venv, data, secrets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_html/              # Manual downloads (HTML files)
â”‚   â””â”€â”€ processed/             # Extracted JSON & parquet/CSV
â”œâ”€â”€ notebooks/                 # Exploration & visual checks
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Paths, Airtable keys (loaded from .env)
â”‚   â”œâ”€â”€ extract_jobs.py        # Playwright + Nuxt JSON extractor
â”‚   â”œâ”€â”€ process_jobs.py        # Combine & clean data
â”‚   â”œâ”€â”€ airtable/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ push.py            # Upload dataframe to Airtable
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_extract.py
    â””â”€â”€ test_process.py
```

---

## ğŸ› ï¸ TechÂ Stack

| Layer            | Choice                             | Rationale                              |
| ---------------- | ---------------------------------- | -------------------------------------- |
| Language         | PythonÂ 3.11Â (Conda)                | Familiar, rich scraping & data libs    |
| HeadlessÂ browser | Playwright                         | Fast, modern, handles JS (Nuxt) pages  |
| Parsing          | `json`, `pandas`                   | Lightweight transform & analysis       |
| Storage          | Airtable (REST / `pyairtable`)     | Noâ€‘code visualisation & Kanban tagging |
| Devâ€‘Ops          | GitHub + GitHubÂ Actions (optional) | Version control & scheduled CI runs    |

---

## ğŸ—“ï¸ Project Plan (Milestones)

| Phase | Deliverable                               | TargetÂ Date |
| ----- | ----------------------------------------- | ----------- |
| 0     | Repo & Conda env initialised              | Â TÂ +Â 0Â days |
| 1     | Manual download procedure documented      | Â TÂ +Â 1Â day  |
| 2     | `extract_jobs.py` Playwright prototype    | Â TÂ +Â 3Â days |
| 3     | Data merge & cleaning to single dataframe | Â TÂ +Â 5Â days |
| 4     | Airtable schema & `push.py` integration   | Â TÂ +Â 6Â days |
| 5     | Endâ€‘toâ€‘end smoke test & logging           | Â TÂ +Â 7Â days |
| 6     | README & wiki refinements                 | Â TÂ +Â 8Â days |

---

## âœ…Â TodoÂ Checklist

- [ ] Initialise Git repo & push to GitHub
- [ ] Create Conda environment (`environment.yml`)
- [ ] Draft `.gitignore` & `.env.example`
- [ ] Document savedâ€‘search URLs
- [ ] Prototype `extract_jobs.py` with Playwright
- [ ] Extract JSON and save perâ€‘page files
- [ ] Build `process_jobs.py` to combine JSON âœ DataFrame
- [ ] Design Airtable base & fields
- [ ] Implement `airtable/push.py`
- [ ] Write unit tests (`tests/`)
- [ ] Run first full workflow & verify Airtable rows
- [ ] Automate cleanup of HTML files
- [ ] Refine README, add screenshots, publish demo GIF

---

## âœ¨Â GettingÂ Started (QuickÂ Run)

```bash
# 1. Clone and set up env
git clone https://github.com/<yourâ€‘user>/upwork_scraper_airtable.git
cd upwork_scraper_airtable
conda env create -f environment.yml
conda activate upwork-scraper

# 2. Place your downloaded HTML in data/raw_html/
# 3. Run extraction & push
python -m src.extract_jobs
python -m src.process_jobs
python -m src.airtable.push
```

> **Tip:** Add `AIRTABLE_API_KEY` and `AIRTABLE_BASE_ID` to your `.env` before pushing.

---

Happy scrapingÂ â€” and may your Upwork feed finally feel like **your** feed!



# üï∏Ô∏è Upwork Job Scraper ‚ûú Airtable¬†Visual¬†Dashboard

> **One‚Äëclick insight into the freelance projects that matter to me.**

---

## üöÄ Motivation & Objectives

* **Cut through noise¬†& decision fatigue**: Upwork‚Äôs interface buries the gigs I actually care about; this project surfaces them instantly.
* **Respect Upwork¬†ToS**: All HTML pages are **manually** downloaded‚Äîno automated scraping of the live site.
* **Centralised review**: Push cleaned job data to Airtable so I can tag, score, and track leads in one place.
* **Daily habit loop**: A lightweight workflow I can run every morning in under 5¬†minutes.

---

## üîÑ Proposed Workflow (Visual)

```mermaid
flowchart TD
    Step1[Create saved searches list] --> Step2[Manual HTML download daily]
    Step2 --> Step3[Render pages with Playwright and extract Nuxt JSON]
    Step3 --> Step4[Combine JSON into dataframe]
    Step4 --> Step5[Push postings to Airtable]
    Step5 --> Step6[Delete HTML files]
```

```mermaid
graph TD
    A[Upwork HTML Downloads] --> B[extract_jobs.py]
    B --> C[process_jobs.py]
    C --> D[Airtable Push: Top N rows]
    C --> E[Supabase Archive: All rows]

    D --> F[Review in Airtable UI]
    F -->|Tag as Rejected| G[Move to Supabase]
    F -->|Tag as Keep| H[Track progress]
```

---

## üóÇÔ∏è Project & File Structure

```text
upwork_scraper_airtable/
‚îú‚îÄ‚îÄ environment.yml            # Conda environment spec
‚îú‚îÄ‚îÄ README.md                  # <- you are here
‚îú‚îÄ‚îÄ .gitignore                 # Exclude venv, data, secrets
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw_html/              # Manual downloads (HTML files)
‚îÇ   ‚îî‚îÄ‚îÄ processed/             # Extracted JSON & parquet/CSV
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ walkthrough.md         # Guide on how to use the tool
‚îú‚îÄ‚îÄ notebooks/                 # Exploration & visual checks
‚îÇ   ‚îî‚îÄ‚îÄ exploration.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Paths, Airtable keys (loaded from .env)
‚îÇ   ‚îú‚îÄ‚îÄ extract_jobs.py        # Playwright + Nuxt JSON extractor
‚îÇ   ‚îú‚îÄ‚îÄ process_jobs.py        # Combine & clean data
‚îÇ   ‚îú‚îÄ‚îÄ airtable/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archive.py         # archives flagged records from Airtable
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ push.py            # Upload dataframe to Airtable
‚îÇ   ‚îú‚îÄ‚îÄ supabase/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ push.py            # Upload dataframe to Supabase
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ logger.py
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_extract.py
    ‚îî‚îÄ‚îÄ test_process.py
```

---

## üõ†Ô∏è Tech¬†Stack

| Layer            | Choice                             | Rationale                              |
| ---------------- | ---------------------------------- | -------------------------------------- |
| Language         | Python¬†3.11¬†(Conda)                | Familiar, rich scraping & data libs    |
| Headless¬†browser | Playwright                         | Fast, modern, handles JS (Nuxt) pages  |
| Parsing          | `json`, `pandas`                   | Lightweight transform & analysis       |
| Storage          | Airtable (REST / `pyairtable`)     | No‚Äëcode visualisation & Kanban tagging |
| Archive Storage  | supabase                           | long term storage of job records       |
| Dev‚ÄëOps          | GitHub + GitHub¬†Actions (optional) | Version control & scheduled CI runs    |

---

## üóìÔ∏è Project Plan (Milestones)

| Phase | Deliverable                               | Target¬†Date |
| ----- | ----------------------------------------- | ----------- |
| 0     | Repo & Conda env initialised              | ¬†T¬†+¬†0¬†days |
| 1     | Manual download procedure documented      | ¬†T¬†+¬†1¬†day  |
| 2     | `extract_jobs.py` Playwright prototype    | ¬†T¬†+¬†3¬†days |
| 3     | Data merge & cleaning to single dataframe | ¬†T¬†+¬†5¬†days |
| 4     | Airtable schema & `push.py` integration   | ¬†T¬†+¬†6¬†days |
| 5     | Supabase schema & `push.py` integration   | ¬†T¬†+¬†7¬†days |
| 6     | Airtable archive workflow                 | ¬†T¬†+¬†8¬†days |
| 7     | End‚Äëto‚Äëend smoke test & logging           | ¬†T¬†+¬†9¬†days |
| 8     | README & wiki refinements                 | ¬†T¬†+¬†10¬†days |

---

## ‚úÖ¬†Todo¬†Checklist

- [x] Initialise Git repo & push to GitHub
- [x] Create Conda environment (`environment.yml`)
- [x] Draft `.gitignore` & `.env.example`
- [ ] Document saved‚Äësearch URLs
- [x] Prototype `extract_jobs.py` with Playwright
- [x] Extract JSON and save per‚Äëpage files
- [x] Build `process_jobs.py` to combine JSON ‚ûú DataFrame
- [x] Design Airtable base & fields
- [x] Implement `airtable/push.py`
- [ ] Design Supabase base & fields
- [ ] Implement `supabase/push.py`
- [ ] Develop workflow and archive automations `airtable/archive.py`
- [ ] Write unit tests (`tests/`)
- [ ] Run first full workflow & verify Airtable rows
- [ ] Automate cleanup of HTML files
- [ ] Refine README, add screenshots, publish demo GIF

---

## ‚ú®¬†Getting¬†Started (Quick¬†Run)

```bash
# 1. Clone and set up env
git clone https://github.com/<your‚Äëuser>/upwork_scraper_airtable.git
cd upwork_scraper_airtable
conda env create -f environment.yml
conda activate upwork-scraper

# 2. Place your downloaded HTML in data/raw_html/
# 3. Run extraction & push
python -m src.extract_jobs
python -m src.process_jobs
python -m src.airtable.push
```

> **Tip:** Add `AIRTABLE_API_KEY` and `AIRTABLE_BASE_ID` to your `.env` before pushing.

---

Happy scraping¬†‚Äî and may your Upwork feed finally feel like **your** feed!


## High level operation data flows
- Local System
  - python scripts for
    - Generating & opening URLs
    - Extracting data from HTML downloads
    - Processing data
    - Uploading data to Supabase
    - Maintaining Airtable
- Upwork/WebScrapBook
  - Download HTMLs
- Airtable
  - Lead tracker for New and Shortlisted Jobs
  - Lead
    - Shortlisted > Proposal > Interview > Contract > Complete
    - Discarded
- Notion?
  - Deal tracker from proposal to deal
- Supabase
  - Long term storage
    - Scrape requests - records each html file (query) that is saved
      - Schema
        - Search ID
        - Query timestamp
        - Upload timestamp
        - Query
        - Page
        - filepath
        - processed
    - Jobs - record unique jobs
      - Schema As per JSON @/Users/jslade/Documents/GitHub/upwork_scraper/data/processed/combined.json
    - Search Results - records unique combination of Jobs and Search
      - Schema
        - SearchID
        - JobID
        - proposalsTier (Track the lifetime of a job on the platform and monitor proposals growth)

### Workflow
1. Generate & Open search URLS
2. Save pages using WebScrapBook
3. Process downloads - check searchID does not exist before proceeding
4. Extract & process jobs
5. Push jobs to supabase - UPSERT job_id, UPDATE search record with total jobs and new jobs
6. Refresh Airtable with Updated jobs, archive the triaged jobs
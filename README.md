# ðŸ•¸ï¸ Upwork Job Analysis âžœ AirtableÂ VisualÂ Dashboard

> **Oneâ€‘click insight into the freelance projects that matter to me.**

---

## ðŸš€ Motivation & Objectives

* **Cut through noiseÂ & decision fatigue**: Upworkâ€™s interface buries the gigs I actually care about; this project surfaces them instantly.
* **Respect UpworkÂ ToS**: All HTML pages are **manually** downloadedâ€”no automated scraping of the live site.
* **Centralised review**: Push cleaned job data to Airtable so I can tag, score, and track leads in one place.
* **Daily habit loop**: A lightweight workflow I can run every morning in under 5Â minutes.

---

## ðŸ”„ Proposed Workflow (Visual)

```mermaid
flowchart TD
    Step1[Create saved searches list] --> Step2[Manual HTML download daily]
    Step2 --> Step3[Render pages with Playwright and extract Nuxt JSON]
    Step3 --> Step4[Combine JSON into dataframe]
    Step4 --> Step5[Push postings to Airtable]
    Step5 --> Step6[Delete HTML files]
```

```mermaid
graph TD
    A[Upwork HTML Downloads] --> B[extract_jobs.py]
    B --> C[process_jobs.py]
    C --> D[Airtable Push: Top N rows]
    C --> E[Supabase Archive: All rows]

    D --> F[Review in Airtable UI]
    F -->|Tag as Rejected| G[Move to Supabase]
    F -->|Tag as Keep| H[Track progress]
```

---

## ðŸ—‚ï¸ Project & File Structure

The project is organized into a modular and scalable structure, separating concerns into logical components.

```
upwork_scraper/
â”œâ”€â”€ .env.example             # Example environment variables file
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ environment.yml          # Conda environment definition
â”œâ”€â”€ pytest.ini               # Pytest configuration
â”œâ”€â”€ README.md                # Project README
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/           # Stores processed JSON job data
â”‚   â”œâ”€â”€ raw_html/            # Stores manually downloaded raw HTML files
â”‚   â”œâ”€â”€ temp/                # Temporary files (e.g., single HTML extraction output)
â”‚   â””â”€â”€ search_urls.yml      # YAML file for managing search URLs
â”œâ”€â”€ database/
â”‚   â””â”€â”€ schemas/             # SQL schemas for database tables (e.g., Supabase)
â”‚       â”œâ”€â”€ 01_create_scrape_requests_table.sql
â”‚       â”œâ”€â”€ 02_create_jobs_table.sql
â”‚       â””â”€â”€ 03_create_search_results_table.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ walkthrough.md       # Original project walkthrough guide
â”‚   â””â”€â”€ PROJECT_DOCS.md      # This comprehensive documentation file
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ html_parsing_test.ipynb # Jupyter notebook for HTML parsing tests
â”œâ”€â”€ src/
â”‚   â””â”€â”€ upwork_scraper/      # Main Python package for the application
â”‚       â”œâ”€â”€ __init__.py      # Package initializer
â”‚       â”œâ”€â”€ cli.py           # Central Command-Line Interface (CLI) entry point
â”‚       â”œâ”€â”€ config.py        # Centralized application configuration and path constants
â”‚       â”œâ”€â”€ scraping.py      # Functions for HTML parsing, job extraction, and URL handling
â”‚       â”œâ”€â”€ processing.py    # Functions for data flattening, metadata parsing, and data preparation
â”‚       â”œâ”€â”€ connectors/      # Sub-package for external service integrations
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ airtable.py  # Logic for Airtable API interactions (sync, push, update)
â”‚       â”‚   â””â”€â”€ supabase.py  # Logic for Supabase API interactions (insert, update, push)
â”‚       â””â”€â”€ utils.py         # General utility functions (e.g., file cleanup)
â””â”€â”€ tests/                   # Unit and integration tests
    â”œâ”€â”€ test_processing.py
    â””â”€â”€ test_scraping.py
```

---

## ðŸ› ï¸ TechÂ Stack

| Layer            | Choice                             | Rationale                              |
| ---------------- | ---------------------------------- | -------------------------------------- |
| Language         | PythonÂ 3.11Â (Conda)                | Familiar, rich scraping & data libs    |
| HeadlessÂ browser | Playwright                         | Fast, modern, handles JS (Nuxt) pages  |
| Parsing          | `json`, `pandas`                   | Lightweight transform & analysis       |
| Storage          | Airtable (REST / `pyairtable`)     | Noâ€‘code visualisation & Kanban tagging |
| Archive Storage  | supabase                           | long term storage of job records       |
| Devâ€‘Ops          | GitHub + GitHubÂ Actions (optional) | Version control & scheduled CI runs    |

---

## ðŸ—“ï¸ Project Plan (Milestones)

| Phase | Deliverable                               | TargetÂ Date |
| ----- | ----------------------------------------- | ----------- |
| 0     | Repo & Conda env initialised              | Â TÂ +Â 0Â days |
| 1     | Manual download procedure documented      | Â TÂ +Â 1Â day  |
| 2     | `extract_jobs.py` Playwright prototype    | Â TÂ +Â 3Â days |
| 3     | Data merge & cleaning to single dataframe | Â TÂ +Â 5Â days |
| 4     | Airtable schema & `push.py` integration   | Â TÂ +Â 6Â days |
| 5     | Supabase schema & `push.py` integration   | Â TÂ +Â 7Â days |
| 6     | Airtable archive workflow                 | Â TÂ +Â 8Â days |
| 7     | Endâ€‘toâ€‘end smoke test & logging           | Â TÂ +Â 9Â days |
| 8     | README & wiki refinements                 | Â TÂ +Â 10Â days |

---

## âœ…Â TodoÂ Checklist

- [x] Initialise Git repo & push to GitHub
- [x] Create Conda environment (`environment.yml`)
- [x] Draft `.gitignore` & `.env.example`
- [x] Document savedâ€‘search URLs
- [x] Prototype `extract_jobs.py` with Playwright
- [x] Extract JSON and save perâ€‘page files
- [x] Build `process_jobs.py` to combine JSON âžœ DataFrame
- [x] Design Airtable base & fields
- [x] Implement `airtable/push.py`
- [x] Design Supabase base & fields
- [x] Implement `supabase/push.py`
- [x] Develop workflow and archive automations `airtable/archive.py`
- [x] Write unit tests (`tests/`)
- [x] Run first full workflow & verify Airtable rows
- [x] Automate cleanup of HTML files
- [ ] Refine README, add screenshots, publish demo GIF

---

For comprehensive technical documentation and detailed walkthroughs, see [PROJECT_DOCS.md](docs/PROJECT_DOCS.md).

Happy scrapingÂ â€” and may your Upwork feed finally feel like **your** feed!


## High level operation data flows
- Local System
  - python scripts for
    - Generating & opening URLs
    - Extracting data from HTML downloads
    - Processing data
    - Uploading data to Supabase
    - Maintaining Airtable
- Upwork/WebScrapBook
  - Download HTMLs
- Airtable
  - Lead tracker for New and Shortlisted Jobs
  - Lead
    - Shortlisted > Proposal > Interview > Contract > Complete
    - Discarded
- Notion?
  - Deal tracker from proposal to deal
- Supabase
  - Long term storage
    - Scrape requests - records each html file (query) that is saved
      - Schema
        - Search ID
        - Query timestamp
        - Upload timestamp
        - Query
        - Page
        - filepath
        - processed
    - Jobs - record unique jobs
      - Schema As per JSON @/Users/jslade/Documents/GitHub/upwork_scraper/data/processed/combined.json
    - Search Results - records unique combination of Jobs and Search
      - Schema
        - SearchID
        - JobID
        - proposalsTier (Track the lifetime of a job on the platform and monitor proposals growth)

### Workflow
1. Generate & Open search URLS - not yet implemented
2. Save pages using WebScrapBook - done
3. Process downloads using main.py and push to supabase - done
  - extract_jobs from the html into json files
  - process_jobs to transform and load the data into supabase
4. Cleanup and load jobs into Airtable - TODO next
  - Query jobs in Airtable and fetch the airtable status for each job_id. Update airtable_status in supabase jobs schema
  - Flush all lead and discarded jobs from airtable
  - push top 50 jobs from supabase to airtable where Status is lead or blank based on priority logic to be defined.
